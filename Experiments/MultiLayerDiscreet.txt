import numpy as np

# Define inputs and desired outputs
inputs = np.array([
    [1, -2, 0, -1],
    [0, 1.5, -0.5, -1],
    [-1, 1, 0.5, -1]
])

# Define desired outputs for multiple output neurons
desired = np.array([
    [-1, 1],  # Desired output for first input
    [-1, -1], # Desired output for second input
    [1, -1]   # Desired output for third input
])

# Initialize weights for multiple outputs (2 outputs in this case)
weights = np.random.rand(2, inputs.shape[1]) * 0.1
learning_rate = 0.1
epochs = 100

# Sigmoid activation function
def activation(x):
    return 1 / (1 + np.exp(-x))

# Training loop
for epoch in range(epochs):
    for i in range(len(inputs)):
        input_vector = inputs[i]
        target_vector = desired[i]
        
        # Compute the weighted sum and apply activation for each output neuron
        weighted_sum = np.dot(weights, input_vector)
        net_output = activation(weighted_sum)
        
        # Compute the error (difference between target and output)
        error = target_vector - net_output
        
        # Update weights for each output neuron
        for j in range(weights.shape[0]):  # Update weights for each output neuron
            weights[j] += learning_rate * error[j] * input_vector

print("Trained weights:")
print(weights)

# Define test function
def test_perceptron(weights, inputs):
    outputs = []
    for input_vector in inputs:
        weighted_sum = np.dot(weights, input_vector)
        output = activation(weighted_sum)
        output = np.where(output>=0.5,1,-1)
        outputs.append(output)
    return np.array(outputs)

# Test the model
test_outputs = test_perceptron(weights, inputs)

# Print the results
print("Testing multiple-output perceptron:")
print("Desired outputs:\n", desired)
print("Predicted outputs:\n", test_outputs)